#!/bin/bash
version="0.6.1"

# Same time zone with US stock market
export TZ="America/New_York"

# Maximum compression level
export GZIP="-9"

# Arguments that will be passed from command line
s3_bucket=""
script_dir=`pwd`
working_dir=`pwd`
end_date=`date +%Y%m%d`

# Required executables
aws_bin="aws"
crawler_bin="pystock-crawler"

print_version() {
    echo "pystock-dailycrawl v$version"
}

print_help() {
    print_version
    echo "Usage:"
    echo "  dailycrawl [-b S3_BUCKET] [-c SCRIPT_DIR] [-w WORKING_DIR] [-s START_DATE] [-e END_DATE] [-o OUTPUT_DIR]"
    echo "  dailycrawl (-h | --help)"
    echo "  dailycrawl (-v | --version)"
}

error() {
    echo "Error: $1"
}

# Check if a directory exists
check_dir() {
    if [ ! -d "$1" ]; then
        error "directory '$1' does not exist"
        exit 1
    fi
}

# Check if an executable exists
check_bin() {
    hash $1 2>/dev/null || { error "'$1' cannot be found in PATH: $PATH"; exit 2; }
}

# Add a directory into PATH if it does not exist in PATH
add_path() {
    if [ -d "$1" ] && [[ ":$PATH:" != *":$1:"* ]]; then
        PATH="${PATH:+"$PATH:"}$1"
    fi
}

# Check if a date (YYYYMMDD) is a holiday
is_holiday() {
    # Saturday or Sunday?
    weekday=`date -d $1 +%u`
    if [ "$weekday" = 6 -o "$weekday" = 7 ]; then
        echo $1
    else
        # US holiday?
        year=`date -d $1 +%Y`
        holiday_file="$script_dir/holidays/$year.txt"
        cat $holiday_file | grep $1
    fi
}

# Get the previous date of a date (YYYYMMDD)
get_prev_date() {
    date -d "$1 -1 day" +%Y%m%d
}

# Some paths may be only added after login, which makes some commands such as 'aws'
# unavailable in crontab's @reboot. Add them here just in case.
add_path "/usr/local/bin"
add_path "/sbin"

# Parse command arguments
while test "$1" != ""
do
    case $1 in
        --help|-h)
            print_help
            exit 0
            ;;
        --version|-v)
            print_version
            exit 0
            ;;
        --s3-bucket|-b)
            shift
            s3_bucket="$1"
            ;;
        --script-dir|-c)
            shift
            script_dir="$1"
            ;;
        --working-dir|-w)
            shift
            working_dir="$1"
            ;;
        --start-date|-s)
            shift
            start_date="$1"
            ;;
        --end-date|-e)
            shift
            end_date="$1"
            ;;
        --output-dir|-o)
            shift
            output_dir="$1"
            ;;
        *)
            shift
            ;;
    esac
done

if [ ! "$s3_bucket" ]; then
    error "must specify an S3 bucket"
    exit 1
fi

# Check directories
check_dir $working_dir
check_dir $script_dir

# Check if required commands are available
check_bin $aws_bin
check_bin $crawler_bin

# Do nothing if end_date is holiday
if [ `is_holiday $end_date` ]; then
    echo "'$end_date' is holiday, stock market is closed"
    exit 0
fi

# If start_date is not specified, set it to the first previous
# non-holiday (counting backward from end_date)
if [ ! "$start_date" ]; then
    start_date=`get_prev_date $end_date`
    while [ `is_holiday $start_date` ]
    do
        start_date=`get_prev_date $start_date`
    done
fi

# Set output directory
date_dir=`date -d $end_date +%Y/%m/%d`
if [ ! "$output_dir" ]; then
    output_dir="$working_dir/$date_dir"
    mkdir -p $output_dir
else
    check_dir $output_dir
fi

echo "Start date: $start_date"
echo "End date: $end_date"
echo "S3 bucket: $s3_bucket"
echo "Script directory: $script_dir"
echo "Working directory: $working_dir"
echo "Output directory: $output_dir"

orig_dir=`pwd`
cd $working_dir

echo "Crawling symbols"
$crawler_bin symbols NYSE,NASDAQ -o $output_dir/symbols.txt -l $output_dir/symbols.log

echo "Crawling prices"
$crawler_bin prices $output_dir/symbols.txt -o $output_dir/prices.csv -s $start_date -e $end_date -l $output_dir/prices.log

echo "Crawling reports"
$crawler_bin reports $output_dir/symbols.txt -o $output_dir/reports.csv -s $start_date -l $output_dir/reports.log

cd $output_dir

echo "Compressing $output_dir/symbols.*"
tar -czf symbols.tar.gz symbols.txt symbols.log

echo "Compressing $output_dir/prices.*"
tar -czf prices.tar.gz prices.csv prices.log

echo "Compressing $output_dir/reports.*"
tar -czf reports.tar.gz reports.csv reports.log

echo "Uploading $output_dir/symbols.tar.gz"
$aws_bin s3 cp symbols.tar.gz s3://$s3_bucket/$date_dir/symbols.tar.gz

echo "Uploading $output_dir/prices.tar.gz"
$aws_bin s3 cp prices.tar.gz s3://$s3_bucket/$date_dir/prices.tar.gz

echo "Uploading $output_dir/reports.tar.gz"
$aws_bin s3 cp reports.tar.gz s3://$s3_bucket/$date_dir/reports.tar.gz

cd $orig_dir
echo "Done"
